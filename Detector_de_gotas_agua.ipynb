{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBTGRO7HkVcd"
   },
   "source": [
    "## MODEL py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQPmsm-ZWI1u"
   },
   "outputs": [],
   "source": [
    "# Pix2Pix tutorial\n",
    "#https://www.youtube.com/watch?v=YsrMGcgfETY\n",
    "# Pix2Pix paper\n",
    "#https://arxiv.org/pdf/1611.07004.pdf\n",
    "# Paper U-Net: Convolutional Networks for BiomedicalImage Segmentation\n",
    "#https://arxiv.org/pdf/1505.04597.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "#import skimage.transform as trans\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from ktf import backend as keras   .add(lambda(lambda x: x ** 2))\n",
    "\n",
    "def downsample(n_filers, kernel=3, apply_batch=True):\n",
    "    initializer = tf.random_normal_initializer(0,0.02)\n",
    "    result = Sequential()\n",
    "    result.add(Conv2D(n_filers,\n",
    "                    kernel,\n",
    "                    strides = 2,\n",
    "                    padding = 'same',\n",
    "                    kernel_initializer = initializer,\n",
    "                    use_bias = not apply_batch,))\n",
    "    if apply_batch:\n",
    "        result.add(BatchNormalization())\n",
    "    result.add(ReLU())\n",
    "    return result\n",
    "\n",
    "def upsample(n_filers, kernel = 3, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0,0.02)\n",
    "    result = Sequential()\n",
    "    result.add(Conv2DTranspose(n_filers,\n",
    "                            kernel,\n",
    "                            strides = 2,\n",
    "                            padding = 'same',\n",
    "                            kernel_initializer = initializer,\n",
    "                            use_bias = False,))\n",
    "    result.add(BatchNormalization())\n",
    "    if apply_dropout:\n",
    "        result.add(Dropout(0.5))\n",
    "    result.add(ReLU())\n",
    "    return result\n",
    "\n",
    "def Segmenter(pretrained_weights = None,input_size = (256,256,3)):\n",
    "    inputs = Input(input_size)\n",
    "    down_stack=[\n",
    "              #Lambda(lambda x: x/127.5-1),           #Normaliza los valores de los pixeles\n",
    "              downsample(64,apply_batch=False),  #128,\n",
    "              downsample(128),                   #64\n",
    "              downsample(256),                   #32\n",
    "              downsample(512),                   #16\n",
    "              downsample(512),                   #8\n",
    "              downsample(512),                   #4\n",
    "              downsample(512),                   #2\n",
    "              downsample(512),                   #1\n",
    "    ]\n",
    "    up_stack=[\n",
    "              upsample(512,apply_dropout=True), #2,\n",
    "              upsample(512,apply_dropout=True), #4\n",
    "              upsample(512,apply_dropout=True), #8\n",
    "              upsample(512),                   #16\n",
    "              upsample(256),                   #32\n",
    "              upsample(128),                   #64\n",
    "              upsample(64),                   #128\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0,0.02)\n",
    "    last = Conv2DTranspose(filters =1,\n",
    "                         kernel_size = 3,\n",
    "                         strides = 2,\n",
    "                         padding='same',\n",
    "                         kernel_initializer=initializer,\n",
    "                         activation = 'sigmoid',)\n",
    "    x = inputs\n",
    "    s =[]\n",
    "    concat = Concatenate()\n",
    "    # CODIFICADOR\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        s.append(x)\n",
    "    s = s[::-1][1:]\n",
    "    # DECODIFICADOR\n",
    "    for up,sk in zip(up_stack,s):\n",
    "        x = up(x)\n",
    "        x = concat([x,sk])\n",
    "    # Capa final de binarizaci贸n\n",
    "    last =  last(x)\n",
    "    # Generando el modelo\n",
    "    model = Model(inputs=inputs, outputs = last)\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    # load pretrained weights\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIUDYa0HkYuk"
   },
   "source": [
    "## DATA py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "import skimage.io as io\n",
    "from os import walk\n",
    "import skimage.transform as trans\n",
    "\n",
    "Sky = [128,128,128]\n",
    "Building = [128,0,0]\n",
    "Pole = [192,192,128]\n",
    "Road = [128,64,128]\n",
    "Pavement = [60,40,222]\n",
    "Tree = [128,128,0]\n",
    "SignSymbol = [192,128,128]\n",
    "Fence = [64,64,128]\n",
    "Car = [64,0,128]\n",
    "Pedestrian = [64,64,0]\n",
    "Bicyclist = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "COLOR_DICT = np.array([Sky, Building, Pole, Road, Pavement,\n",
    "                          Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "\n",
    "\n",
    "def adjustData(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "            #for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
    "            #index = np.where(mask == i)\n",
    "            #index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
    "            #new_mask[index_mask] = 1\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "        mask[mask > 0.5] = 1\n",
    "        mask[mask <= 0.5] = 0\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "\n",
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"rgb\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\" ImageDataGenerator.flow_from_directory\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n",
    "\n",
    "\n",
    "\n",
    "def testGenerator(test_path,num_image = 30,target_size = (256,256),flag_multi_class = False,as_gray = True):\n",
    "    f=[]\n",
    "    for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    for name in f:\n",
    "        img = io.imread(os.path.join(test_path,name),as_gray = as_gray)\n",
    "        img = img / 255\n",
    "        img = cv2.resize(img,target_size[::-1])\n",
    "        #img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "        img = np.reshape(img,(1,)+img.shape)\n",
    "        yield img\n",
    "\n",
    "\n",
    "def geneTrainNpy(image_path,mask_path,flag_multi_class = False,num_class = 2,image_prefix = \"image\",mask_prefix = \"mask\",image_as_gray = True,mask_as_gray = True):\n",
    "    image_name_arr = glob.glob(os.path.join(image_path,\"%s*.png\"%image_prefix))\n",
    "    image_arr = []\n",
    "    mask_arr = []\n",
    "    for index,item in enumerate(image_name_arr):\n",
    "        img = io.imread(item,as_gray = image_as_gray)\n",
    "        img = np.reshape(img,img.shape + (1,)) if image_as_gray else img\n",
    "        mask = io.imread(item.replace(image_path,mask_path).replace(image_prefix,mask_prefix),as_gray = mask_as_gray)\n",
    "        mask = np.reshape(mask,mask.shape + (1,)) if mask_as_gray else mask\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        image_arr.append(img)\n",
    "        mask_arr.append(mask)\n",
    "    image_arr = np.array(image_arr)\n",
    "    mask_arr = np.array(mask_arr)\n",
    "    return image_arr,mask_arr\n",
    "\n",
    "\n",
    "def labelVisualize(num_class,color_dict,img):\n",
    "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i,:] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "\n",
    "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2,orig_size=(255,255)):\n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        img = cv2.resize(img,orig_size[::-1])\n",
    "        io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)\n",
    "        \n",
    "#Post procesamiento de la mascara obtenida        \n",
    "def post_mask(img,orig_size=(255,255),flag_multi_class = False,num_class = 2, threshold = 0.5 ):\n",
    "    img = labelVisualize(num_class,COLOR_DICT,img) if flag_multi_class else img[:,:,0]\n",
    "    img[img<threshold] = 0\n",
    "    img[img>=threshold] = 255\n",
    "    img, area = post_mask_util(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = cv2.resize(img,orig_size[::-1], interpolation = cv2.INTER_AREA) #resize en opencv esta invertido\n",
    "    img = img.astype('uint8') \n",
    "    return img,area\n",
    "\n",
    "def post_mask_util(img_bin, kernel_=(3,3)):\n",
    "    # Clean up\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, kernel_)\n",
    "    # Fill small gaps\n",
    "    img_bin = cv2.morphologyEx(img_bin, cv2.MORPH_CLOSE, kernel)\n",
    "    # Remove specks\n",
    "    img_bin = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, kernel)\n",
    "    # Contando la cantidad de puntos detectados en la mascara de 256x256\n",
    "    area = np.sum(img_bin == 255)\n",
    "    \n",
    "    return img_bin , area\n",
    "# Preprocesamiento para el tratado de la mascara\n",
    "def pre_mask(img_orig, crop = (.3 , .87, .38,.56),size=(256,256)):\n",
    "    (alto, ancho, chs) = img_orig.shape\n",
    "    y_ini = int(alto*crop[0])\n",
    "    y_fin = int(alto*crop[1])\n",
    "    x_ini = int(ancho*crop[2])\n",
    "    x_fin = int(ancho*crop[3])\n",
    "    img_crop = img_orig[y_ini:y_fin,x_ini:x_fin]\n",
    "    img = img_crop.copy()\n",
    "    img = img / 255\n",
    "    img = cv2.resize(img,size, interpolation = cv2.INTER_AREA)\n",
    "    #img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "    img = np.reshape(img,(1,)+img.shape)\n",
    "    return img , img_crop # imagen a introducir en la red , dimensiones originales de la imagen de entrada\n",
    "\n",
    "integer = 0\n",
    "last_time = 0\n",
    "worst_lost = 0\n",
    "#Funci贸n de integraci贸n para la integraci贸n de areas en el tiempo\n",
    "def lost_condition(area, time):\n",
    "    # se toma como medida de referencia una taza de 30FPS\n",
    "    global last_time\n",
    "    global integer\n",
    "    global worst_lost\n",
    "    if area > 30:\n",
    "        integer += area*(time-last_time)\n",
    "    else:\n",
    "        integer = 0 \n",
    "        if worst_lost < integer: \n",
    "            worst_lost = integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time_pre = 1 Time_predic: 390 Time_posp: 2 Indice fuga: 610.571364402771\n",
      "Time_pre = 0 Time_predic: 102 Time_posp: 1 Indice fuga: 676.1361827850342\n",
      "Time_pre = 0 Time_predic: 105 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 111 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 109 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 109 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 102 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 101 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 107 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 0 Time_predic: 109 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 0 Time_predic: 107 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 109 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 107 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 113 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 109 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 125 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 117 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 120 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 115 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 110 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 0 Time_predic: 109 Time_posp: 1 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 110 Time_posp: 1 Indice fuga: 12.216652631759644\n",
      "Time_pre = 0 Time_predic: 108 Time_posp: 2 Indice fuga: 41.65220260620117\n",
      "Time_pre = 1 Time_predic: 107 Time_posp: 2 Indice fuga: 92.72854995727539\n",
      "Time_pre = 0 Time_predic: 115 Time_posp: 2 Indice fuga: 178.10238671302795\n",
      "Time_pre = 1 Time_predic: 123 Time_posp: 2 Indice fuga: 400.39914441108704\n",
      "Time_pre = 1 Time_predic: 129 Time_posp: 0 Indice fuga: 468.5678894519806\n",
      "Time_pre = 1 Time_predic: 112 Time_posp: 2 Indice fuga: 486.55793142318726\n",
      "Time_pre = 0 Time_predic: 111 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 0 Time_predic: 108 Time_posp: 0 Indice fuga: 0\n",
      "Time_pre = 1 Time_predic: 105 Time_posp: 2 Indice fuga: 0\n",
      "Time_pre = 2 Time_predic: 109 Time_posp: 2 Indice fuga: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "model = Segmenter(pretrained_weights =\"segmenter_unet.hdf5\",input_size = (256,256,3))\n",
    "set_ = 0\n",
    "#pos_cam = [(.3 , .78, .38,.56), (.25,.5,.6,.7)]\n",
    "pos_cam = [(.3 , .87, .38,.56), (.25,.5,.6,.7)]\n",
    "pos    = pos_cam[set_]\n",
    "videos = [\"cheves.mp4\", \"cheves2.mp4\"]\n",
    "camara = cv2.VideoCapture(os.path.join(\"img_orig\",videos[set_]))\n",
    "while True and not TRAIN:\n",
    "    last_time = time.time()\n",
    "    (grabbed, frame) = camara.read()\n",
    "    if not grabbed:\n",
    "        break\n",
    "    time_pre_start = time.time()\n",
    "    img, img_crop = pre_mask(frame, crop = pos)\n",
    "    time_pre_end = time.time()\n",
    "    results = model.predict(img)\n",
    "    time_predic_end = time.time()\n",
    "    \n",
    "    rgb_mask, area = post_mask(results[0],orig_size=img_crop.shape[:2],threshold=0.9)\n",
    "    lost_condition(area, time.time())\n",
    "    time_post_end = time.time()\n",
    "    pre = int((time_pre_end - time_pre_start)*1000)\n",
    "    pred = int((time_predic_end - time_pre_end)*1000) \n",
    "    posp = int((time_post_end - time_predic_end)*1000) \n",
    "    print(\"Time_pre = {} Time_predic: {} Time_posp: {} Indice fuga: {}\".format(pre,pred,posp,integer))\n",
    "\n",
    "    img = cv2.addWeighted(rgb_mask, 0.6, img_crop, 0.4, 0)\n",
    "    cv2.imshow(\"origi + Mask\",img)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"s\"):\n",
    "        break\n",
    "camara.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2]\n",
    "a[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsFRRBHHsjT5"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bUqNUpD6sl0s",
    "outputId": "bf04fe49-dee4-402e-a286-5b8ac2dc3725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218 images belonging to 1 classes.\n",
      "Found 218 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0130 05:17:59.551648 140080063682304 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 images belonging to 1 classes.\n",
      "Found 56 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0130 05:17:59.817475 140080063682304 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 700 steps, validate for 400 steps\n",
      "Epoch 1/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9963\n",
      "Epoch 00001: val_loss improved from inf to 0.03447, saving model to segmenter_unet.hdf5\n",
      "700/700 [==============================] - 382s 545ms/step - loss: 0.0090 - accuracy: 0.9963 - val_loss: 0.0345 - val_accuracy: 0.9888\n",
      "Epoch 2/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9963\n",
      "Epoch 00002: val_loss improved from 0.03447 to 0.02467, saving model to segmenter_unet.hdf5\n",
      "700/700 [==============================] - 380s 544ms/step - loss: 0.0090 - accuracy: 0.9963 - val_loss: 0.0247 - val_accuracy: 0.9911\n",
      "Epoch 3/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9963\n",
      "Epoch 00003: val_loss did not improve from 0.02467\n",
      "700/700 [==============================] - 365s 522ms/step - loss: 0.0088 - accuracy: 0.9963 - val_loss: 0.0332 - val_accuracy: 0.9883\n",
      "Epoch 4/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9963\n",
      "Epoch 00004: val_loss did not improve from 0.02467\n",
      "700/700 [==============================] - 375s 536ms/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 0.0263 - val_accuracy: 0.9905\n",
      "Epoch 5/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9964\n",
      "Epoch 00005: val_loss did not improve from 0.02467\n",
      "700/700 [==============================] - 376s 537ms/step - loss: 0.0085 - accuracy: 0.9964 - val_loss: 0.0268 - val_accuracy: 0.9896\n",
      "Epoch 6/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9964\n",
      "Epoch 00006: val_loss improved from 0.02467 to 0.02426, saving model to segmenter_unet.hdf5\n",
      "700/700 [==============================] - 387s 552ms/step - loss: 0.0087 - accuracy: 0.9964 - val_loss: 0.0243 - val_accuracy: 0.9910\n",
      "Epoch 7/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9966\n",
      "Epoch 00007: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 413s 590ms/step - loss: 0.0081 - accuracy: 0.9966 - val_loss: 0.0296 - val_accuracy: 0.9893\n",
      "Epoch 8/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9965\n",
      "Epoch 00008: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 376s 537ms/step - loss: 0.0082 - accuracy: 0.9965 - val_loss: 0.0258 - val_accuracy: 0.9904\n",
      "Epoch 9/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9966\n",
      "Epoch 00009: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 400s 571ms/step - loss: 0.0081 - accuracy: 0.9966 - val_loss: 0.0311 - val_accuracy: 0.9896\n",
      "Epoch 10/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9967\n",
      "Epoch 00010: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 420s 601ms/step - loss: 0.0078 - accuracy: 0.9967 - val_loss: 0.0331 - val_accuracy: 0.9904\n",
      "Epoch 11/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9967\n",
      "Epoch 00011: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 481s 687ms/step - loss: 0.0079 - accuracy: 0.9967 - val_loss: 0.0384 - val_accuracy: 0.9869\n",
      "Epoch 12/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9966\n",
      "Epoch 00012: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 438s 625ms/step - loss: 0.0080 - accuracy: 0.9966 - val_loss: 0.0280 - val_accuracy: 0.9906\n",
      "Epoch 13/25\n",
      "699/700 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9966\n",
      "Epoch 00013: val_loss did not improve from 0.02426\n",
      "700/700 [==============================] - 471s 673ms/step - loss: 0.0079 - accuracy: 0.9966 - val_loss: 0.0304 - val_accuracy: 0.9905\n",
      "Epoch 14/25\n",
      "682/700 [============================>.] - ETA: 10s - loss: 0.0076 - accuracy: 0.9968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0130 06:52:27.508926 140080063682304 callbacks.py:1018] Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-142c76b82a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtboard_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'segmenter_unet.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyGene\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyGene_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtestGene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/robot/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/robot/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/robot/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mcurrent_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     with training_context.on_batch(\n\u001b[0;32m--> 126\u001b[0;31m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0m\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    data_gen_args = dict(rotation_range=0.2,\n",
    "                        width_shift_range=0.05,\n",
    "                        height_shift_range=0.05,\n",
    "                        shear_range=0.05,\n",
    "                        zoom_range=0.05,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest')\n",
    "    myGene = trainGenerator(2,'data/train','img2','label2',data_gen_args,save_to_dir = None)\n",
    "    myGene_val = trainGenerator(2,'data/train','img','label',data_gen_args,save_to_dir = None, seed=45)\n",
    "\n",
    "    #model = Segmenter(input_size = (256,256,3))\n",
    "    model = Segmenter(pretrained_weights =\"segmenter_unet.hdf5\",input_size = (256,256,3))\n",
    "    logdir=\"logs\" \n",
    "    tboard_callback = TensorBoard(log_dir=logdir)\n",
    "    model_checkpoint = ModelCheckpoint('segmenter_unet.hdf5', monitor='val_loss',verbose=1, save_best_only=True) \n",
    "    model.fit(myGene,validation_data=myGene_val, steps_per_epoch=700,epochs=25,callbacks=[model_checkpoint,tboard_callback],validation_steps=400)      \n",
    "\n",
    "    testGene = testGenerator(\"data/test\",as_gray = False)\n",
    "    results = model.predict_generator(testGene,30,verbose=1)\n",
    "    saveResult(\"data/test/tested\",results,orig_size=(169,115))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir logs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter(pretrained_weights =\"segmenter_unet.hdf5\",input_size = (256,256,3))\n",
    "testGene = testGenerator(\"data/test\",as_gray = False)\n",
    "results = model.predict_generator(testGene,28,verbose=1)\n",
    "saveResult(\"data/test/tested\",results,orig_size=(169,115))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Detector de gotas&agua.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
